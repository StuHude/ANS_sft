# å‘ç°å…³é”®Bugï¼šg_pixel_valuesç”Ÿæˆé”™è¯¯

## Bugä½ç½®

**trainer.py line 398-405**:
```python
# âŒ é”™è¯¯å®ç°
images_1024 = F.interpolate(
    images,  # è¿™æ˜¯448 ImageNet normalizedï¼
    size=(1024, 1024),
    mode='bilinear',
    align_corners=False
)
# SAM2's preprocess_image expects [0, 255] range
images_1024 = (images_1024 * 255.0).clamp(0, 255)  # âŒ å®Œå…¨é”™è¯¯ï¼
```

## ä¸ºä»€ä¹ˆæ˜¯Bug

### SAM2çš„preprocess_imageæœŸæœ›è¾“å…¥

æŸ¥çœ‹`sam2_train.py` line 63-69:
```python
def preprocess_image(self, image: torch.Tensor) -> torch.Tensor:
    image = image / 255.  # âœ… æœŸæœ›è¾“å…¥[0, 255]
    img_mean = torch.tensor(self.img_mean, ...)  # (0.485, 0.456, 0.406)
    img_std = torch.tensor(self.img_std, ...)    # (0.229, 0.224, 0.225)
    image -= img_mean
    image /= img_std
    return image  # è¿”å›ImageNet normalized
```

### å½“å‰å®ç°çš„é—®é¢˜

**Step 1**: `images`æ˜¯448 ImageNet normalized
- å€¼èŒƒå›´: çº¦[-2.5, 2.5]
- ä¾‹å¦‚æŸä¸ªåƒç´ å€¼: -1.2

**Step 2**: `images_1024 = images * 255.0`
- -1.2 * 255 = -306
- å€¼èŒƒå›´: çº¦[-637.5, 637.5]

**Step 3**: `clamp(0, 255)`
- -306 â†’ 0 (æˆªæ–­ï¼)
- æ‰€æœ‰è´Ÿå€¼éƒ½è¢«æˆªæ–­ä¸º0
- æ‰€æœ‰>255çš„å€¼è¢«æˆªæ–­ä¸º255

**ç»“æœ**: **å›¾åƒä¿¡æ¯å®Œå…¨è¢«ç ´åï¼**

### æ­£ç¡®çš„åšæ³•

åº”è¯¥å…ˆ**ånormalize**ï¼Œå†è½¬ä¸º[0, 255]:
```python
# Step 1: ånormalize (ä»ImageNet normalizedè½¬å›[0, 1])
mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(images.device)
std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(images.device)
images_unnorm = images * std + mean  # ç°åœ¨æ˜¯[0, 1]

# Step 2: Resizeåˆ°1024
images_1024 = F.interpolate(images_unnorm, size=(1024, 1024), mode='bilinear')

# Step 3: è½¬ä¸º[0, 255]
g_pixel_values = (images_1024 * 255.0).clamp(0, 255)
```

### ä½†è¿™æ ·è¿˜æ˜¯æœ‰é—®é¢˜

ä»448ånormalize â†’ resizeåˆ°1024ï¼Œä»ç„¶ä¼šæŸå¤±ç²¾åº¦ï¼Œå› ä¸ºï¼š
1. åŸå§‹å›¾åƒå¯èƒ½æ˜¯æ›´é«˜åˆ†è¾¨ç‡ï¼ˆå¦‚SAVçš„512Ã—512ï¼‰
2. å…ˆresizeåˆ°448å†resizeåˆ°1024ï¼Œç»è¿‡äº†ä¸¤æ¬¡é™é‡‡æ ·+å‡é‡‡æ ·

---

## ä½ çš„å»ºè®®æ˜¯æ­£ç¡®çš„

### åº”è¯¥åœ¨Datasetä¸­è¿”å›å¤šç§æ ¼å¼

å‚è€ƒSa2VAåŸå§‹å®ç° (RefCOCO_Dataset.py line 195-199):
```python
# 1. åŸå§‹å›¾åƒ â†’ 1024 [0, 255] for SAM2
g_image = np.array(image)  # PIL â†’ numpy (uint8, 0-255)
g_image = self.extra_image_processor.apply_image(g_image)  # DirectResize to 1024
g_pixel_values = torch.from_numpy(g_image).permute(2, 0, 1)  # (3, 1024, 1024), [0, 255]

# 2. åŸå§‹å›¾åƒ â†’ 448 ImageNet normalized for InternVL
pixel_values = self.transformer(image)  # (3, 448, 448) normalized
```

### æˆ‘ä»¬åº”è¯¥åšçš„ä¿®æ”¹

**Datasetè¿”å›**:
```python
{
    'pixel_values': (3, 448, 448) ImageNet normalized,  # for InternVL
    'g_pixel_values': (3, 1024, 1024) [0, 255] uint8,  # for SAM2
    'prompt_masks': (16, 16) boolean,                   # for visual prompt
    'masks': (1024, 1024) [0, 1] float,                 # for GT loss
}
```

**è®­ç»ƒä»£ç ä½¿ç”¨**:
```python
# Loop 1 (maskâ†’caption, EMA + trainable):
- pixel_values (448)
- prompt_masks (16Ã—16)

# Loop 2 (captionâ†’mask, trainable):
- pixel_values (448)
- g_pixel_values (1024)
- masks (1024, GT)
```

---

## ä¸ºä»€ä¹ˆè¿™æ ·æ›´å¥½

### ä¼˜åŠ¿1: é¿å…å¤šæ¬¡resize

**å½“å‰é”™è¯¯å®ç°**:
```
åŸå§‹å›¾åƒ â†’ 1024 normalized â†’ 448 normalized â†’ 1024 [0, 255] âŒ
         (dataset)           (è®­ç»ƒloop1)       (è®­ç»ƒloop2)
```

**æ­£ç¡®å®ç°**:
```
åŸå§‹å›¾åƒ â†’ 448 normalized  (dataset, ç”¨äºInternVL)
        â†’ 1024 [0, 255]    (dataset, ç”¨äºSAM2)
```

### ä¼˜åŠ¿2: é¿å…ånormalizeçš„ç²¾åº¦æŸå¤±

ImageNet normalizationä¸å¯é€†:
```python
# normalize
x_norm = (x - mean) / std

# ånormalize
x_recovered = x_norm * std + mean
# ç”±äºæµ®ç‚¹ç²¾åº¦é—®é¢˜ï¼Œx_recovered â‰  x
```

### ä¼˜åŠ¿3: ä¿æŒä¸Sa2VAåŸå§‹å®ç°ä¸€è‡´

RefCOCOã€ReVOSç­‰æ•°æ®é›†éƒ½æ˜¯åœ¨Datasetä¸­ç”Ÿæˆä¸¤ç§æ ¼å¼ã€‚

---

## ä¿®å¤æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: Datasetè¿”å›å¤šç§æ ¼å¼ (æ¨è)

ä¿®æ”¹`dataset_builder.py`:
```python
class SAVDatasetWrapper(Dataset):
    def __init__(self, ...):
        # Image transform for InternVL (448 normalized)
        self.image_transform_448 = T.Compose([
            T.ToPILImage(),
            T.Resize((448, 448), interpolation=T.InterpolationMode.BILINEAR),
            T.ToTensor(),
            T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)
        ])

        # Image transform for SAM2 (1024 [0, 255])
        self.image_transform_1024 = T.Compose([
            T.ToPILImage(),
            T.Resize((1024, 1024), interpolation=T.InterpolationMode.BILINEAR),
            T.ToTensor(),
            # ä¸åšnormalizeï¼Œä¿æŒ[0, 1]ï¼Œåé¢è½¬ä¸º[0, 255]
        ])

        # Mask transform for prompt (16Ã—16)
        self.mask_transform_16 = T.Compose([
            T.ToPILImage(mode='L'),
            T.Resize((16, 16), interpolation=T.InterpolationMode.NEAREST),
            T.ToTensor(),
        ])

        # Mask transform for GT (1024)
        self.mask_transform_1024 = T.Compose([
            T.ToPILImage(mode='L'),
            T.Resize((1024, 1024), interpolation=T.InterpolationMode.NEAREST),
            T.ToTensor(),
        ])

    def __getitem__(self, idx):
        ...
        return {
            'pixel_values': image_448,        # (3, 448, 448) normalized
            'g_pixel_values': (image_1024 * 255).byte(),  # (3, 1024, 1024) [0, 255] uint8
            'prompt_masks': mask_16,          # (16, 16) [0, 1]
            'masks': mask_1024,               # (1024, 1024) [0, 1]
        }
```

### æ–¹æ¡ˆ2: åœ¨è®­ç»ƒä»£ç ä¸­æ­£ç¡®ånormalize (ä¸´æ—¶æ–¹æ¡ˆ)

å¦‚æœæš‚æ—¶ä¸æƒ³ä¿®æ”¹Datasetï¼Œè‡³å°‘è¦ä¿®å¤å½“å‰çš„bug:

```python
# trainer.py
def unnormalize_image(images, mean, std):
    """Reverse ImageNet normalization"""
    mean = torch.tensor(mean).view(1, 3, 1, 1).to(images.device)
    std = torch.tensor(std).view(1, 3, 1, 1).to(images.device)
    return images * std + mean

# åœ¨éœ€è¦g_pixel_valuesçš„åœ°æ–¹:
images_unnorm = unnormalize_image(images, IMAGENET_MEAN, IMAGENET_STD)  # [0, 1]
images_1024 = F.interpolate(images_unnorm, size=(1024, 1024), mode='bilinear')
g_pixel_values_input = (images_1024 * 255.0).clamp(0, 255)  # [0, 255]
```

---

## æ€»ç»“

1. âœ… **ä½ çš„ç†è§£å®Œå…¨æ­£ç¡®**
   - Loop 1éœ€è¦: pixel_values (448) + prompt_masks (16Ã—16)
   - Loop 2éœ€è¦: pixel_values (448) + g_pixel_values (1024) + masks (GT)

2. âŒ **å½“å‰å®ç°æœ‰ä¸¥é‡bug**
   - ç›´æ¥å¯¹normalizedå›¾åƒä¹˜ä»¥255æ˜¯é”™è¯¯çš„
   - ä¼šç ´åå›¾åƒä¿¡æ¯

3. âœ… **ä½ çš„å»ºè®®æ˜¯æœ€ä½³æ–¹æ¡ˆ**
   - Datasetè¿”å›å¤šç§æ ¼å¼
   - é¿å…å¤šæ¬¡resizeå’Œånormalize
   - ä¸Sa2VAåŸå§‹å®ç°ä¸€è‡´

4. ğŸ”§ **éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶**
   - `dataset_builder.py`: è¿”å›4ç§æ ¼å¼
   - `trainer.py`: åˆ é™¤é”™è¯¯çš„g_pixel_valuesç”Ÿæˆä»£ç 
   - `pseudo_gumbel_core.py`: ä½¿ç”¨datasetæä¾›çš„æ ¼å¼

---

**å»ºè®®ç«‹å³ä¿®å¤è¿™ä¸ªbugï¼Œå¦åˆ™ç¬¬äºŒé˜¶æ®µçš„maskç”Ÿæˆè´¨é‡ä¼šå¾ˆå·®ï¼**
