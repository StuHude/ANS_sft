# æ•°æ®é›†å®é™…ä½¿ç”¨æƒ…å†µ - å‡†ç¡®åˆ†æ

## âŒ é—®é¢˜ç¡®è®¤

### 1. å®é™…ä½¿ç”¨çš„æ•°æ®é›†ï¼š**åªæœ‰3ä¸ªï¼Œä¸æ˜¯4ä¸ª**

```
âœ“ Dataset 1: SAV      - /data/xyc/formed_data/npz
âœ“ Dataset 2: SA1B     - /data/xyc/mhx/SA1b/OpenDataLab___SA-1B/raw
âœ“ Dataset 3: RefCOCO  - ./data/ref_seg
âœ— Dataset 4: OpenImage - ./data/openimages (ä¸å­˜åœ¨)
```

### 2. LengthGroupedSamplerï¼š**åŸå§‹è®­ç»ƒä½¿ç”¨äº†ï¼Œä½†æˆ‘ä»¬æ²¡æœ‰**

**åŸå§‹Sa2VAé…ç½®** (sa2va_4b.py):
```python
train_dataloader = dict(
    batch_size=batch_size,
    num_workers=dataloader_num_workers,
    dataset=train_dataset,
    sampler=dict(
        type=LengthGroupedSampler,
        length_property='modality_length',  # â† å…³é”®ï¼
        per_device_batch_size=batch_size * accumulative_counts
    ),
    collate_fn=dict(type=video_lisa_collate_fn)
)
```

**æˆ‘ä»¬å½“å‰çš„é…ç½®**:
```python
train_dataloader = DataLoader(
    train_dataset,
    batch_size=args.batch_size,
    shuffle=True,  # â† åªæ˜¯ç®€å•éšæœºï¼
    num_workers=args.num_workers,
    collate_fn=collate_fn_mask_caption,
    pin_memory=True,
)
```

### 3. æ•°æ®é›†é‡‡æ ·å‚æ•°ï¼š**æˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨repeatså’Œæƒé‡**

**åŸå§‹é…ç½®çš„æ•°æ®é›†é‡å¤**:
```python
train_dataset=dict(
    type=ConcatDataset, datasets=[
        # RefCOCOç³»åˆ—é‡å¤4æ¬¡ï¼
        refcoco_segm_dataset, refcoco_plus_segm_dataset, refcocog_segm_dataset,
        refcoco_segm_dataset, refcoco_plus_segm_dataset, refcocog_segm_dataset,
        refcoco_segm_dataset, refcoco_plus_segm_dataset, refcocog_segm_dataset,
        refcoco_segm_dataset, refcoco_plus_segm_dataset, refcocog_segm_dataset,
        # GranDfé‡å¤10æ¬¡
        glamm_grandf_dataset,  # repeats=10
        # å…¶ä»–æ•°æ®é›†...
    ]
)
```

**æˆ‘ä»¬å½“å‰çš„é…ç½®**:
- SAV: 1æ¬¡
- SA1B: 1æ¬¡ï¼ˆæµ‹è¯•æ—¶é™åˆ¶500æ ·æœ¬ï¼‰
- RefCOCO: 1æ¬¡
- æ²¡æœ‰ä»»ä½•repeatsæˆ–æƒé‡è°ƒæ•´

---

## âœ… éœ€è¦çš„ä¿®å¤

### ä¿®å¤1: æ·»åŠ LengthGroupedSampler

**ä¸ºä»€ä¹ˆéœ€è¦**:
1. **åŸå§‹è®­ç»ƒä½¿ç”¨äº†** - åº”è¯¥ä¿æŒä¸€è‡´
2. **æé«˜è®­ç»ƒæ•ˆç‡** - ç›¸ä¼¼é•¿åº¦çš„æ ·æœ¬åœ¨åŒä¸€batchï¼Œå‡å°‘paddingæµªè´¹
3. **ç¨³å®šå†…å­˜ä½¿ç”¨** - é¿å…batché—´å†…å­˜å·®å¼‚è¿‡å¤§

**å¦‚ä½•å®ç°**:

#### Step 1: åœ¨datasetä¸­æ·»åŠ modality_lengthå±æ€§

```python
# åœ¨ dataset_builder.py ä¸­çš„ SAVDatasetWrapper, SA1BDatasetWrapper, RefCOCODatasetWrapper

class SAVDatasetWrapper(Dataset):
    # ... ç°æœ‰ä»£ç  ...

    @property
    def modality_length(self):
        """Return list of modality lengths for LengthGroupedSampler"""
        # ç®€å•å®ç°ï¼šå›ºå®šé•¿åº¦ï¼ˆå› ä¸ºå›¾åƒéƒ½æ˜¯1024x1024ï¼‰
        # ä½†å¯ä»¥æ ¹æ®captioné•¿åº¦æ¥è®¡ç®—
        return [1024] * len(self)  # æˆ–è€…è®¡ç®—å®é™…çš„åºåˆ—é•¿åº¦

    def __getitem__(self, idx):
        # ... ç°æœ‰ä»£ç  ...
        result = {
            'image1': image1,
            'mask1': mask1.squeeze(0),
            'image2': image2,
            'mask2': mask2.squeeze(0),
            'dataset_type': 'sav',
            'has_paired_frame': True,
            'modality_length': 1024,  # â† æ·»åŠ è¿™ä¸ª
        }
        return result
```

#### Step 2: åœ¨dataloaderä¸­ä½¿ç”¨LengthGroupedSampler

```python
from xtuner.dataset.samplers import LengthGroupedSampler

# åœ¨ train_dual_loop.py çš„ main() å‡½æ•°ä¸­
train_dataloader = DataLoader(
    train_dataset,
    batch_size=args.batch_size,
    # shuffle=True,  # â† ç§»é™¤è¿™ä¸ª
    sampler=LengthGroupedSampler(  # â† æ·»åŠ è¿™ä¸ª
        train_dataset,
        batch_size=args.batch_size,
        world_size=1 if args.local_rank == -1 else torch.distributed.get_world_size(),
        rank=0 if args.local_rank == -1 else args.local_rank,
        seed=42,
    ),
    num_workers=args.num_workers,
    collate_fn=collate_fn_mask_caption,
    pin_memory=True,
)
```

### ä¿®å¤2: æ·»åŠ æ•°æ®é›†é‡‡æ ·æƒé‡ï¼ˆå¯é€‰ï¼‰

å¦‚æœæƒ³è®©æŸäº›æ•°æ®é›†è¢«é‡‡æ ·æ›´å¤šæ¬¡ï¼š

```python
# æ–¹æ¡ˆA: ç®€å•é‡å¤ï¼ˆåƒåŸå§‹é…ç½®ï¼‰
datasets = []
if sav_dir:
    datasets.append(SAVDatasetWrapper(...))
if sa1b_dir:
    datasets.append(SA1BDatasetWrapper(...))
if refcoco_dir:
    # RefCOCOé‡å¤4æ¬¡ï¼ˆåƒåŸå§‹é…ç½®ï¼‰
    for _ in range(4):
        datasets.append(RefCOCODatasetWrapper(...))

# æ–¹æ¡ˆB: ä½¿ç”¨WeightedRandomSamplerï¼ˆæ›´çµæ´»ï¼‰
from torch.utils.data import WeightedRandomSampler

dataset_weights = {
    'sav': 1.0,
    'sa1b': 1.0,
    'refcoco': 4.0,  # RefCOCOæƒé‡4å€
}

sample_weights = []
for i in range(len(train_dataset)):
    sample = train_dataset[i]
    weight = dataset_weights.get(sample['dataset_type'], 1.0)
    sample_weights.append(weight)

sampler = WeightedRandomSampler(
    weights=sample_weights,
    num_samples=len(train_dataset),
    replacement=True
)
```

### ä¿®å¤3: æ·»åŠ ç¬¬4ä¸ªæ•°æ®é›† - OpenImage

**é€‰é¡¹A: è·³è¿‡OpenImageï¼ˆå¦‚æœæ•°æ®ä¸å¯ç”¨ï¼‰**
- å½“å‰çŠ¶æ€ï¼šå·²å®ç°è‡ªåŠ¨è·³è¿‡
- è®­ç»ƒä¼šæ­£å¸¸è¿›è¡Œï¼Œåªç”¨3ä¸ªæ•°æ®é›†

**é€‰é¡¹B: è·å–OpenImageæ•°æ®**

1. **ä¸‹è½½OpenImage v7æ•°æ®**:
```bash
# åˆ›å»ºç›®å½•
mkdir -p ./data/openimages/{images,masks}/train

# ä¸‹è½½æ•°æ®ï¼ˆéœ€è¦ç¡®è®¤å…·ä½“æ•°æ®æºï¼‰
# è¿™éƒ¨åˆ†éœ€è¦ç”¨æˆ·æä¾›OpenImageæ•°æ®çš„ä¸‹è½½æ–¹å¼
```

2. **é…ç½®æ•°æ®é›†è·¯å¾„**:
```bash
# ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨
./data/openimages/
â”œâ”€â”€ train-annotations-object-segmentation.csv
â”œâ”€â”€ oidv7-class-descriptions.csv
â”œâ”€â”€ images/train/
â””â”€â”€ masks/train/
```

---

## ğŸ“Š æ¨èé…ç½®

### é…ç½®1: æœ€å°ä¿®æ”¹ï¼ˆ3ä¸ªæ•°æ®é›† + LengthGroupedSamplerï¼‰

**é€‚ç”¨åœºæ™¯**: OpenImageæ•°æ®ä¸å¯ç”¨ï¼Œå¿«é€Ÿå¼€å§‹è®­ç»ƒ

**ä¿®æ”¹**:
1. âœ… æ·»åŠ LengthGroupedSampler
2. âœ… ä¿æŒ3ä¸ªæ•°æ®é›†ï¼ˆSAV, SA1B, RefCOCOï¼‰
3. âœ… ä¸ä½¿ç”¨æ•°æ®é›†é‡å¤

**é¢„æœŸæ•ˆæœ**:
- æ›´ç¨³å®šçš„è®­ç»ƒ
- æ›´å°‘çš„paddingæµªè´¹
- ä¸åŸå§‹é…ç½®æ›´æ¥è¿‘

### é…ç½®2: å®Œæ•´é…ç½®ï¼ˆ4ä¸ªæ•°æ®é›† + LengthGroupedSampler + æƒé‡ï¼‰

**é€‚ç”¨åœºæ™¯**: OpenImageæ•°æ®å¯ç”¨ï¼Œè¿½æ±‚æœ€ä½³æ•ˆæœ

**ä¿®æ”¹**:
1. âœ… æ·»åŠ LengthGroupedSampler
2. âœ… æ·»åŠ OpenImageæ•°æ®é›†
3. âœ… ä½¿ç”¨æ•°æ®é›†æƒé‡ï¼ˆRefCOCOÃ—4ï¼‰
4. âœ… ä½¿ç”¨æ›´å¤§çš„batch size

**é¢„æœŸæ•ˆæœ**:
- æœ€æ¥è¿‘åŸå§‹è®­ç»ƒé…ç½®
- æ›´å¥½çš„æ€§èƒ½
- æ›´å¹³è¡¡çš„æ•°æ®åˆ†å¸ƒ

---

## ğŸ”§ å…·ä½“å®æ–½æ­¥éª¤

### ç«‹å³æ‰§è¡Œï¼ˆæ¨èï¼‰

**Step 1: æ·»åŠ modality_lengthå±æ€§**
```bash
# ä¿®æ”¹ dataset_builder.py
# åœ¨æ¯ä¸ªDataset wrapperçš„__getitem__ä¸­æ·»åŠ  'modality_length' å­—æ®µ
```

**Step 2: ä¿®æ”¹dataloaderé…ç½®**
```bash
# ä¿®æ”¹ train_dual_loop.py
# å°† shuffle=True æ”¹ä¸ºä½¿ç”¨ LengthGroupedSampler
```

**Step 3: é‡æ–°æµ‹è¯•**
```bash
docker exec -w /data/xyc/ANS vlm-env bash test_dual_loop.sh
```

### å¯é€‰æ‰§è¡Œ

**æ·»åŠ æ•°æ®é›†é‡å¤**ï¼ˆå¦‚æœæƒ³æ¨¡ä»¿åŸå§‹é…ç½®ï¼‰:
- RefCOCOé‡å¤4æ¬¡
- æˆ–ä½¿ç”¨WeightedRandomSamplerè®¾ç½®æƒé‡

---

## ğŸ“ å¯¹æ¯”æ€»ç»“

| é¡¹ç›® | åŸå§‹Sa2VAé…ç½® | å½“å‰æˆ‘ä»¬çš„é…ç½® | å·®å¼‚ |
|------|--------------|--------------|------|
| æ•°æ®é›†æ•°é‡ | 15+ datasets | 3 datasets | âŒ å°‘å¾ˆå¤š |
| LengthGroupedSampler | âœ… ä½¿ç”¨ | âŒ æœªä½¿ç”¨ | âŒ ç¼ºå¤± |
| æ•°æ®é›†é‡å¤ | âœ… RefCOCOÃ—4 | âŒ æ— é‡å¤ | âŒ ç¼ºå¤± |
| modality_length | âœ… æœ‰ | âŒ æ—  | âŒ ç¼ºå¤± |
| OpenImage | N/A | âŒ ä¸å­˜åœ¨ | âš ï¸ æ•°æ®é—®é¢˜ |

**ç»“è®º**: æˆ‘ä»¬çš„é…ç½®ä¸åŸå§‹è®­ç»ƒå·®å¼‚è¾ƒå¤§ï¼Œ**åº”è¯¥æ·»åŠ LengthGroupedSampler**ã€‚

---

## âš¡ å¿«é€Ÿä¿®å¤ä»£ç 

æˆ‘å¯ä»¥ç«‹å³ä¸ºæ‚¨å®ç°ï¼š
1. æ·»åŠ modality_lengthåˆ°æ‰€æœ‰dataset
2. ä¿®æ”¹dataloaderä½¿ç”¨LengthGroupedSampler
3. ï¼ˆå¯é€‰ï¼‰æ·»åŠ æ•°æ®é›†æƒé‡é…ç½®

æ˜¯å¦éœ€è¦æˆ‘ç°åœ¨å°±å®æ–½è¿™äº›ä¿®å¤ï¼Ÿ
