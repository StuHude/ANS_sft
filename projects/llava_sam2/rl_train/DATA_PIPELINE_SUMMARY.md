# GARæ•°æ®å¤„ç†Pipelineè¯´æ˜

## æ¶æ„è®¾è®¡

å‚è€ƒäº†`describe_anything_referring_dataset.py`çš„å®ç°ï¼Œæˆ‘ä»¬é‡‡ç”¨**åˆ†ç¦»å¼è®¾è®¡**ï¼š

1. **åŸå§‹æ•°æ®åŠ è½½** (`dataset_gar.py`)
2. **æ¨¡å‹è¾“å…¥é¢„å¤„ç†** (`data_preprocessor.py`)

è¿™ç§è®¾è®¡çš„ä¼˜åŠ¿ï¼š
- RLè®­ç»ƒéœ€è¦åŸå§‹æ•°æ®ï¼ˆè®¡ç®—rewardï¼Œå¦‚IOUéœ€è¦åŸå§‹maskï¼‰
- æ¨¡å‹æ¨ç†éœ€è¦é¢„å¤„ç†æ•°æ®
- ä¸¤ä¸ªéœ€æ±‚å¯ä»¥ç‹¬ç«‹æ»¡è¶³

## ç»„ä»¶è¯¦æƒ…

### 1. åŸå§‹æ•°æ®åŠ è½½å™¨ (`dataset_gar.py`)

```python
from projects.llava_sam2.rl_train.dataset_gar import GraspAnyRegionDataset

dataset = GraspAnyRegionDataset(
    local_data_dir="/data/xiaoyicheng/Sa2VA/data/GAR",
    parts_to_load=None  # None = è‡ªåŠ¨åŠ è½½æ‰€æœ‰Part
)

sample = dataset[0]
# Returns:
# {
#     'image': PIL.Image (RGB),
#     'mask': numpy.ndarray (H, W, bool),
#     'caption': str,
#     'category': str,
#     'image_id': str
# }
```

**åŠŸèƒ½ï¼š**
- âœ… ä»Arrowæ–‡ä»¶ç›´æ¥åŠ è½½
- âœ… RLE maskè§£ç 
- âœ… ä»conversationsæå–caption
- âœ… æ”¯æŒå¤šPartè‡ªåŠ¨æ‹¼æ¥

### 2. æ•°æ®é¢„å¤„ç†å™¨ (`data_preprocessor.py`)

```python
from projects.llava_sam2.rl_train.data_preprocessor import Sa2VADataPreprocessor

preprocessor = Sa2VADataPreprocessor()

# å‡†å¤‡mask->captionä»»åŠ¡çš„è¾“å…¥
model_input = preprocessor.prepare_for_model(
    image=sample['image'],
    mask=sample['mask'],
    caption=sample['caption'],
    task="mask_to_caption"
)

# Returns:
# {
#     'pixel_values': torch.Tensor (1, 3, 448, 448),
#     'prompt_masks': torch.Tensor (1, 16, 16),
#     'vp_overall_mask': torch.Tensor (1,),
#     'prompt_text': str,
#     'region_pixels': [K],
#     'gt_caption': str
# }
```

**åŠŸèƒ½ï¼š**
- âœ… å›¾åƒresizeåˆ°448Ã—448å¹¶normalizeï¼ˆImageNet mean/stdï¼‰
- âœ… Maskèšåˆåˆ°16Ã—16 tokenç½‘æ ¼ï¼ˆä½¿ç”¨adaptive_avg_pool2dï¼‰
- âœ… æ„é€ ç‰¹æ®Štokenæ ¼å¼ï¼š`<image> There are 1 part regions in the picture: region1<vp><IMG_CONTEXT>*K</vp>.\n{instruction}`
- âœ… æ”¯æŒmask->captionå’Œcaption->maskä¸¤ç§ä»»åŠ¡

## å…³é”®é¢„å¤„ç†æ­¥éª¤

### å›¾åƒé¢„å¤„ç†
```python
transforms.Compose([
    transforms.Resize((448, 448), interpolation=InterpolationMode.BICUBIC),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.229)),
])
```

### Maské¢„å¤„ç†ï¼ˆæœ€å…³é”®ï¼ï¼‰
```python
# 1. è½¬ä¸ºtorch tensor (1, H, W)
mask_tensor = torch.from_numpy(mask.astype(np.float32)).unsqueeze(0)

# 2. Resizeåˆ°å›¾åƒå°ºå¯¸ (1, 448, 448)
mask_resizer = transforms.Resize((448, 448), interpolation=InterpolationMode.NEAREST)
mask_tensor = mask_resizer(mask_tensor)

# 3. èšåˆåˆ°tokenç½‘æ ¼ (1, 16, 16)
pooled = F.adaptive_avg_pool2d(mask_tensor, (16, 16))

# 4. äºŒå€¼åŒ–
prompt_masks = (pooled > 0.5).to(torch.uint8)

# 5. è®¡ç®—regionä¸­çš„tokenæ•°é‡K
region_pixels = [int(prompt_masks[0].sum().item())]
```

### æ–‡æœ¬æ„é€ 
```python
# Mask->Captionä»»åŠ¡:
prompt = "<image> There are 1 part regions in the picture: region1<vp><IMG_CONTEXT>*181</vp>.\nPlease generate a detailed description for the given image region."

# Caption->Maskä»»åŠ¡:
prompt = "<image> {caption}\nPlease segment the described region."
```

## åœ¨RLè®­ç»ƒä¸­çš„ä½¿ç”¨

### Rollouté˜¶æ®µï¼ˆç”Ÿæˆï¼‰
```python
# 1. ä»dataloaderè·å–åŸå§‹æ•°æ®
sample = dataset[i]

# 2. é¢„å¤„ç†ä¸ºæ¨¡å‹è¾“å…¥
model_input = preprocessor.prepare_for_model(
    image=sample['image'],
    mask=sample['mask'],
    task="mask_to_caption"
)

# 3. æ¨¡å‹æ¨ç†ï¼ˆéœ€è¦ä¸Sa2VAæ¨¡å‹é›†æˆï¼‰
# generated_caption = model.generate(
#     pixel_values=model_input['pixel_values'],
#     prompt_masks=model_input['prompt_masks'],
#     prompt_text=model_input['prompt_text']
# )
```

### Rewardè®¡ç®—é˜¶æ®µ
```python
# ä½¿ç”¨åŸå§‹æ•°æ®è®¡ç®—reward
from projects.llava_sam2.rl_train.reward_functions import compute_iou, compute_meteor

# IOU reward (éœ€è¦åŸå§‹mask)
iou = compute_iou(sample['mask'], generated_mask)

# METEOR reward (éœ€è¦åŸå§‹caption)
meteor = compute_meteor(sample['caption'], generated_caption)
```

## ä¸describe_anything_referring_datasetçš„å¯¹æ¯”

| é¡¹ç›® | describe_anything | æˆ‘ä»¬çš„å®ç° | è¯´æ˜ |
|------|-------------------|-----------|------|
| å›¾åƒå¤„ç† | âœ… (448, 448) | âœ… (448, 448) | ç›¸åŒ |
| Maskç½‘æ ¼ | âœ… (16, 16) | âœ… (16, 16) | ç›¸åŒ |
| ç‰¹æ®Štoken | âœ… | âœ… | ç›¸åŒæ ¼å¼ |
| Tokenize | âœ… (video_lisa_encode_fn) | âŒ | åœ¨RLè®­ç»ƒè„šæœ¬ä¸­å¤„ç† |
| Template | âœ… (template_map_fn) | âŒ | åœ¨RLè®­ç»ƒè„šæœ¬ä¸­å¤„ç† |

**è¯´æ˜ï¼š**
- describe_anythingè¿”å›å®Œå…¨tokenizedçš„æ•°æ®ï¼Œç›´æ¥å¯ä»¥é€å…¥æ¨¡å‹è®­ç»ƒ
- æˆ‘ä»¬çš„å®ç°è¿”å›**é¢„å¤„ç†ä½†æœªtokenize**çš„æ•°æ®ï¼Œå› ä¸ºï¼š
  1. RLè®­ç»ƒéœ€è¦åŸå§‹æ•°æ®è®¡ç®—reward
  2. Tokenizationåœ¨RLè®­ç»ƒè„šæœ¬ä¸­æ›´çµæ´»

## æµ‹è¯•ç»“æœ

```bash
# æµ‹è¯•åŸå§‹æ•°æ®åŠ è½½
python test_gar_quick.py
# âœ“ 3108 samples loaded
# âœ“ Image, mask, caption all correct

# æµ‹è¯•é¢„å¤„ç†å™¨
python test_preprocessor.py
# âœ“ Image: (3, 448, 448)
# âœ“ Mask: (1, 16, 16) with 181 tokens
# âœ“ Prompt text formatted correctly
```

## æ–‡ä»¶æ¸…å•

```
projects/llava_sam2/rl_train/
â”œâ”€â”€ dataset_gar.py              # åŸå§‹æ•°æ®åŠ è½½å™¨
â”œâ”€â”€ data_preprocessor.py        # æ•°æ®é¢„å¤„ç†å™¨
â”œâ”€â”€ test_gar_quick.py          # æµ‹è¯•åŸå§‹æ•°æ®åŠ è½½
â”œâ”€â”€ test_preprocessor.py       # æµ‹è¯•é¢„å¤„ç†å™¨
â”œâ”€â”€ reward_functions.py        # Rewardå‡½æ•°ï¼ˆIOU, METEORï¼‰
â”œâ”€â”€ ema_model.py              # EMAæ¨¡å‹
â””â”€â”€ DATA_PIPELINE_SUMMARY.md  # æœ¬æ–‡æ¡£
```

## ä¸‹ä¸€æ­¥

ç°åœ¨æ•°æ®pipelineå·²å®Œå…¨å‡†å¤‡å¥½ï¼Œå¯ä»¥è¿›è¡Œï¼š
1. âœ… åŸå§‹æ•°æ®åŠ è½½
2. âœ… Sa2VAæ ¼å¼é¢„å¤„ç†
3. ğŸ”„ é›†æˆåˆ°RLè®­ç»ƒè„šæœ¬ï¼ˆéœ€è¦tokenizerå’Œmodelï¼‰
4. ğŸ”„ å®ç°LLM judge reward
5. ğŸ”„ å®Œæ•´çš„RLè®­ç»ƒå¾ªç¯
