from pycocotools import mask as maskUtils
import os
from PIL import Image
import numpy as np
import json
import torch
import cv2
from torchvision import transforms
from tqdm import tqdm
from torch.utils.data import DataLoader
from collections import OrderedDict

class SA1BDataset:
    def __init__(self, dataset_dir, ids=None, annotation_dir='js', image_dir='img',
                 min_object=0, target_size=(1024, 1024), transform=None, max_samples=None,
                 cache_images: bool = False, max_cache_images: int = 0):
        """
        å‚æ•°è¯´æ˜ï¼š
        target_size: ç»Ÿä¸€è¾“å‡ºå°ºå¯¸ (height, width)
        transform: å¯è‡ªå®šä¹‰ï¼Œä½†å¿…é¡»åŒ…å«Resizeæ“ä½œ
        """
        # åˆå§‹åŒ–æ ¸å¿ƒå‚æ•°
        self.dataset_dir = dataset_dir
        self.min_object = min_object
        self.target_size = target_size
        self.annotation_dir = annotation_dir
        self.image_dir = image_dir

        # è‡ªåŠ¨æ„å»ºåŒ…å«Resizeçš„transform
        base_transform = [
            transforms.Resize(target_size),
            transforms.ToTensor()
        ]
        self.transform = transform or transforms.Compose(base_transform)

        # åŠ è½½æ–‡ä»¶åˆ—è¡¨
        if ids is None:
            all_files = sorted(os.listdir(os.path.join(dataset_dir, annotation_dir)))
            if max_samples is not None:
                all_files = all_files[:max_samples]
            ids = [f.replace(".json", "") for f in all_files]

        # åˆå§‹åŒ–æ ·æœ¬è·¯å¾„
        self.samples = [
            (os.path.join(dataset_dir, image_dir, f"{id}.jpg"),
             os.path.join(dataset_dir, annotation_dir, f"{id}.json"))
            for id in ids
        ]

        # å»¶è¿ŸåŠ è½½å‚æ•°
        self.indices = None
        self.cache_images = bool(cache_images) and int(max_cache_images) > 0
        self.max_cache_images = int(max_cache_images) if int(max_cache_images) > 0 else 0
        self.img_cache = OrderedDict() if self.cache_images else None
        self.processed = False

    def _precompute_indices(self):
        """ä¼˜åŒ–çš„å»¶è¿Ÿç´¢å¼•è®¡ç®— - è·³è¿‡é¢„ç»Ÿè®¡ç›´æ¥å¤„ç†"""
        self.indices = []
        print(f"ğŸšš å¼€å§‹é¢„å¤„ç†SA-1Bæ•°æ®é›† ({len(self.samples)} å›¾åƒæ–‡ä»¶)...")
        print("â³ æ­£åœ¨å¤„ç†æ ‡æ³¨... (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œä½†è®­ç»ƒå°†åœ¨åå°ç»§ç»­)")

        # ç›´æ¥å¤„ç†ï¼Œä¸é¢„å…ˆç»Ÿè®¡ï¼ˆèŠ‚çœä¸€åŠæ—¶é—´ï¼‰
        # ä½¿ç”¨æ–‡ä»¶æ•°è€Œä¸æ˜¯annotationæ•°ä½œä¸ºè¿›åº¦æŒ‡ç¤º
        processed_anns = 0
        with tqdm(total=len(self.samples), desc="ğŸ”§ å¤„ç†å›¾åƒ", unit="img") as pbar:
            for img_idx, (img_path, ann_path) in enumerate(self.samples):
                try:
                    # åŠ è½½å›¾åƒåŸå§‹å°ºå¯¸
                    with Image.open(img_path) as img:
                        orig_h, orig_w = img.size[1], img.size[0]

                    # å¤„ç†æ ‡æ³¨
                    with open(ann_path) as f:
                        annotations = json.load(f)['annotations']

                    for ann_idx, ann in enumerate(annotations):
                        try:
                            rle = self._ann_to_rle(ann, orig_h, orig_w)
                            area = maskUtils.area(rle).sum().item()
                            if area >= self.min_object:
                                self.indices.append((img_idx, ann_idx))
                                processed_anns += 1
                        except Exception as e:
                            # é™é»˜è·³è¿‡å•ä¸ªæ ‡æ³¨é”™è¯¯ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                            pass

                    # æ¯1000ä¸ªå›¾åƒæ‰“å°ä¸€æ¬¡è¿›åº¦
                    if (img_idx + 1) % 1000 == 0:
                        print(f"  å·²å¤„ç†: {img_idx + 1}/{len(self.samples)} å›¾åƒ, {processed_anns} æœ‰æ•ˆæ ‡æ³¨")

                except Exception as e:
                    print(f"âš ï¸ å›¾åƒé”™è¯¯: {img_path} - {str(e)}")
                finally:
                    pbar.update(1)

        self.processed = True
        print(f"âœ… é¢„å¤„ç†å®Œæˆ: {len(self.samples)} å›¾åƒ, {len(self.indices):,} æœ‰æ•ˆæ ‡æ³¨")

    def __len__(self):
        if not self.processed:
            self._precompute_indices()
        return len(self.indices)

    def __getitem__(self, index):
        if not self.processed:
            self._precompute_indices()

        img_idx, ann_idx = self.indices[index]
        img_path, ann_path = self.samples[img_idx]

        # åŠ è½½å›¾åƒï¼ˆé»˜è®¤ä¸ç¼“å­˜ï¼›ç¼“å­˜ä¼šå¯¼è‡´RAMä¸æ–­å¢é•¿ç›´è‡³OOMï¼‰
        if self.cache_images:
            if img_idx in self.img_cache:
                img_tensor, (orig_w, orig_h) = self.img_cache.pop(img_idx)
                self.img_cache[img_idx] = (img_tensor, (orig_w, orig_h))
            else:
                with Image.open(img_path) as img_pil:
                    img_pil = img_pil.convert("RGB")
                    orig_w, orig_h = img_pil.size
                    img_tensor = self.transform(img_pil)
                self.img_cache[img_idx] = (img_tensor, (orig_w, orig_h))
                if len(self.img_cache) > self.max_cache_images:
                    self.img_cache.popitem(last=False)
        else:
            with Image.open(img_path) as img_pil:
                img_pil = img_pil.convert("RGB")
                orig_w, orig_h = img_pil.size
                img_tensor = self.transform(img_pil)  # åº”ç”¨å°ºå¯¸å˜æ¢

        # åŠ è½½æ ‡æ³¨å¹¶å¤„ç†mask
        with open(ann_path) as f:
            ann = json.load(f)['annotations'][ann_idx]

        # ç”ŸæˆåŸå§‹mask
        orig_mask = self._ann_to_mask(ann, orig_h, orig_w)  # (h, w)

        # è°ƒæ•´maskå°ºå¯¸ï¼ˆä¿æŒäºŒå€¼ç‰¹æ€§ï¼‰
        mask = cv2.resize(
            orig_mask.astype(np.uint8),
            (self.target_size[1], self.target_size[0]),  # (width, height)
            interpolation=cv2.INTER_NEAREST
        )
        mask = torch.from_numpy(mask).float()

        # è·å–ç±»åˆ«ID
        class_id = torch.tensor(ann['id'], dtype=torch.long)

        return img_tensor, mask, class_id

    def _ann_to_rle(self, ann, height, width):
        """å°†æ ‡æ³¨è½¬æ¢ä¸ºRLEæ ¼å¼"""
        segm = ann['segmentation']
        if isinstance(segm, list):
            rles = maskUtils.frPyObjects(segm, height, width)
            return maskUtils.merge(rles)
        elif isinstance(segm['counts'], list):
            return maskUtils.frPyObjects(segm, height, width)
        return ann['segmentation']

    def _ann_to_mask(self, ann, height, width):
        """ä»RLEç”ŸæˆäºŒå€¼mask"""
        rle = self._ann_to_rle(ann, height, width)
        return maskUtils.decode(rle)
