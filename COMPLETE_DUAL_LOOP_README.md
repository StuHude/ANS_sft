# å®Œæ•´Dual-Loopè®­ç»ƒå®ç°

## âœ… å·²å®Œæˆçš„å®ç°

### æ ¸å¿ƒåŠŸèƒ½

**å®Œæ•´çš„Dual-Loopè®­ç»ƒæµç¨‹**ï¼š

```
Step 1: image + mask â†’ Sa2VA (with visual prompting) â†’ caption
Step 2: image + caption â†’ Sa2VA (referring segmentation) â†’ mask'
Step 3: Loss = segmentation_loss(mask', mask_GT)
```

### å…³é”®ç‰¹æ€§

1. **âœ… Visual Prompting Caption Generation**
   - ä½¿ç”¨`<vp><IMG_CONTEXT>*K</vp>`æ ¼å¼
   - å°†maskè½¬æ¢ä¸º16x16 grid
   - è°ƒç”¨model.generate()ç”ŸæˆçœŸå®caption

2. **âœ… Referring Segmentation**
   - ä½¿ç”¨ç”Ÿæˆçš„captionä½œä¸ºè¾“å…¥
   - é€šè¿‡VideoLLaVASAMModelè®¡ç®—mask loss
   - å®Œæ•´çš„lossï¼šmask_loss + dice_loss + llm_loss

3. **âœ… 4ä¸ªæ•°æ®é›†é›†æˆ**
   - SAV: `/data/xyc/formed_data/npz`
   - SA1B: æ”¯æŒmax_samplesé™åˆ¶
   - OpenImage: å¯é…ç½®
   - RefCOCO: `./data/ref_seg`

4. **âœ… é¢„è®­ç»ƒæƒé‡åŠ è½½**
   - ä»`sa2va_4b_iter152k_fp32.pth`åˆå§‹åŒ–
   - ä½¿ç”¨guess_load_checkpointè‡ªåŠ¨å¤„ç†

5. **âœ… åŸå§‹æ¶æ„å…¼å®¹**
   - ä½¿ç”¨VideoLLaVASAMModelï¼ˆä¸æ˜¯Sa2VAChatModelï¼‰
   - éµå¾ªåŸå§‹æ•°æ®æ ¼å¼
   - ä½¿ç”¨åŸå§‹loss functions

## ğŸš€ å¿«é€Ÿå¼€å§‹

### Step 1: æµ‹è¯•è¿è¡Œï¼ˆæœ‰é™æ•°æ®ï¼Œå¿«é€ŸéªŒè¯ï¼‰

```bash
bash test_dual_loop.sh
```

**é…ç½®**ï¼š
- 4å¡è®­ç»ƒ
- SA1Bé™åˆ¶500ä¸ªæ ·æœ¬
- Batch size 1
- å¿«é€ŸéªŒè¯ä»£ç æ˜¯å¦work

**é¢„æœŸè¾“å‡º**ï¼š
```
Building VideoLLaVASAMModel...
âœ“ Model built
âœ“ Pretrained weights loaded
âœ“ Dataset built: XXXXX total samples
âœ“ Dataloader created: XXXX batches

Epoch 1/1
loss=X.XXX, mask_loss=X.XXX, dice_loss=X.XXX, llm_loss=X.XXX
...
```

### Step 2: å®Œæ•´è®­ç»ƒï¼ˆå…¨éƒ¨æ•°æ®ï¼‰

ç¡®è®¤æµ‹è¯•é€šè¿‡åï¼š

```bash
bash run_dual_loop_full.sh
```

**é…ç½®**ï¼š
- 8å¡åˆ†å¸ƒå¼è®­ç»ƒ
- ä½¿ç”¨å…¨éƒ¨4ä¸ªæ•°æ®é›†
- Batch size 2 per GPU
- Gradient accumulation 4
- Effective batch size = 64

## ğŸ“ å®ç°ç»†èŠ‚

### æ–‡ä»¶ç»“æ„

```
/data/xyc/ANS/
â”œâ”€â”€ projects/llava_sam2/mask_caption_sft/
â”‚   â”œâ”€â”€ train_dual_loop.py          # âœ… å®Œæ•´dual-loopè®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ dataset_builder.py          # âœ… æ•°æ®é›†æ„å»ºï¼ˆå·²æ”¯æŒ4ä¸ªæ•°æ®é›†ï¼‰
â”‚   â””â”€â”€ ...
â”œâ”€â”€ test_dual_loop.sh               # âœ… æµ‹è¯•è„šæœ¬ï¼ˆæœ‰é™æ•°æ®ï¼‰
â”œâ”€â”€ run_dual_loop_full.sh           # âœ… å®Œæ•´è®­ç»ƒè„šæœ¬
â”œâ”€â”€ DUAL_LOOP_TRAINING.md           # ğŸ“– è¯¦ç»†æ–‡æ¡£
â””â”€â”€ COMPLETE_DUAL_LOOP_README.md    # ğŸ“– æœ¬æ–‡æ¡£
```

### æ ¸å¿ƒä»£ç é€»è¾‘

#### 1. Caption Generationï¼ˆStep 1ï¼‰

```python
def generate_caption_from_mask(images, masks):
    # 1. å°†maskè½¬æ¢ä¸º16x16 grid
    prompt_masks = pool_mask_to_grid(masks)  # (B, 16, 16)

    # 2. æ„å»ºvisual promptingè¾“å…¥
    text = f"<img>...</img> region1<vp><IMG_CONTEXT>*K</vp>. Describe this."

    # 3. è°ƒç”¨model.generate()
    outputs = model.generate(
        pixel_values=images_448,
        input_ids=tokenized_text,
        prompt_masks=prompt_masks,
        vp_overall_mask=[True],
        max_new_tokens=128
    )

    # 4. è§£ç caption
    captions = tokenizer.decode(outputs)
    return captions
```

#### 2. Mask Predictionï¼ˆStep 2ï¼‰

```python
def compute_segmentation_loss(images, captions, gt_masks):
    # 1. å‡†å¤‡è¾“å…¥ï¼ˆéµå¾ªåŸå§‹æ ¼å¼ï¼‰
    data = {
        'pixel_values': [images_448[i] for i in range(B)],
        'g_pixel_values': [images_1024[i] for i in range(B)],
        'input_ids': tokenize(f"Segment: {caption}[SEG]"),
        'labels': ...,
        'masks': [gt_masks[i] for i in range(B)],
        'frames_per_batch': [1] * B,
    }

    # 2. Forwardï¼ˆè‡ªåŠ¨è®¡ç®—lossï¼‰
    loss_dict = model(data, mode='loss')

    # 3. è¿”å›
    return {
        'loss': loss_dict['loss_mask'] + loss_dict['loss_dice'] + loss_dict['llm_loss'],
        ...
    }
```

### è®­ç»ƒå‚æ•°

```python
# Model
LoRA: r=128, alpha=256
Frozen: Vision encoder, LLM backbone (except LoRA)
Trainable: LoRA adapters, SAM2 decoder, text_hidden_fcs, mlp1

# Training
Learning rate: 1e-5
Weight decay: 0.05
Max grad norm: 1.0
Batch size: 1 (test), 2 (full)
Gradient accumulation: 4
EMA decay: 0.999

# Loss weights (from VideoLLaVASAMModel config)
loss_mask: 2.0
loss_dice: 0.5
llm_loss: 1.0 (implicit)
```

## ğŸ“Š ç›‘æ§è®­ç»ƒ

### å…³é”®æŒ‡æ ‡

1. **loss**: æ€»æŸå¤±ï¼ˆåº”è¯¥ä¸‹é™ï¼‰
2. **mask_loss**: åƒç´ çº§CE lossï¼ˆåº”è¯¥ä¸‹é™ï¼‰
3. **dice_loss**: Diceç³»æ•°lossï¼ˆåº”è¯¥ä¸‹é™ï¼‰
4. **llm_loss**: è¯­è¨€æ¨¡å‹lossï¼ˆåº”è¯¥ä¸‹é™ï¼‰

### æ­£å¸¸è®­ç»ƒè¡¨ç°

```
Epoch 1/1, Step 0:
loss=3.234, mask_loss=1.567, dice_loss=0.834, llm_loss=0.833

Epoch 1/1, Step 100:
loss=2.456, mask_loss=1.123, dice_loss=0.623, llm_loss=0.710

Epoch 1/1, Step 500:
loss=1.789, mask_loss=0.789, dice_loss=0.401, llm_loss=0.599
```

**å¥½çš„è¿¹è±¡**ï¼š
- LossæŒç»­ä¸‹é™
- Mask losså’Œdice losséƒ½åœ¨æ”¹å–„
- è®­ç»ƒç¨³å®šï¼Œæ²¡æœ‰NaN

**å¼‚å¸¸æƒ…å†µ**ï¼š
- Lossä¸å˜æˆ–ä¸Šå‡ â†’ æ£€æŸ¥å­¦ä¹ ç‡ã€æ•°æ®
- Loss=NaN â†’ æ£€æŸ¥æ¢¯åº¦è£å‰ªã€æ•°æ®é¢„å¤„ç†
- OOM â†’ å‡å°batch sizeæˆ–å¢åŠ gradient accumulation

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. æ¨¡å‹åŠ è½½å¤±è´¥

```bash
# æ£€æŸ¥é¢„è®­ç»ƒæƒé‡è·¯å¾„
ls -lh /data/xiaoyicheng/Sa2VA/pretrained/4B_checkpoint/sa2va_4b_iter152k_fp32.pth/pytorch_model.bin

# æ£€æŸ¥base modelè·¯å¾„
ls -lh ./pretrained/InternVL2_5-4B/
```

#### 2. æ•°æ®é›†åŠ è½½å¤±è´¥

```bash
# æ£€æŸ¥æ•°æ®é›†è·¯å¾„
ls /data/xyc/formed_data/npz/ | head
ls ./data/ref_seg/refcoco/
```

#### 3. Captionç”Ÿæˆå¤±è´¥

**ç—‡çŠ¶**: `Warning: Caption generation failed`

**è§£å†³**ï¼š
1. æ£€æŸ¥model.generate()æ˜¯å¦æ”¯æŒ
2. å¦‚æœä¸æ”¯æŒï¼Œä¼šè‡ªåŠ¨fallbackåˆ°ç®€å•caption
3. è®­ç»ƒä»ç„¶å¯ä»¥ç»§ç»­ï¼ˆä½¿ç”¨ç®€å•captionï¼‰

#### 4. OOM (Out of Memory)

```bash
# é€‰é¡¹1: å‡å°batch size
python ... --batch_size 1 --gradient_accumulation_steps 8

# é€‰é¡¹2: é™åˆ¶SA1B
python ... --sa1b_max_samples 500

# é€‰é¡¹3: å•å¡è®­ç»ƒ
export CUDA_VISIBLE_DEVICES=0
python ... --batch_size 1
```

## ğŸ“ˆ ä¸‹ä¸€æ­¥ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

å½“å‰å®ç°å·²ç»å®Œå…¨æ»¡è¶³éœ€æ±‚ã€‚å¦‚æœæƒ³è¿›ä¸€æ­¥æå‡ï¼Œå¯ä»¥è€ƒè™‘ï¼š

### 1. EMA Teacher Distillation

```python
# ä½¿ç”¨EMAæ¨¡å‹çš„mask predictionä½œä¸ºsoft target
ema_masks = ema_model.predict_mask(image, caption)
loss = loss(student_mask, gt_mask) + 0.5 * loss(student_mask, ema_masks)
```

### 2. Caption Quality Reward

```python
# æ·»åŠ caption quality metric
from reward_functions import combined_caption_reward
caption_reward = combined_caption_reward(generated_caption, gt_caption)
loss = segmentation_loss - 0.1 * caption_reward
```

### 3. Multi-object Support

```python
# æ”¯æŒä¸€å¼ å›¾å¤šä¸ªobjects
for obj_idx, (mask, caption) in enumerate(zip(masks, captions)):
    loss += compute_loss(image, caption, mask)
```

## âœ¨ æ€»ç»“

**å·²å®ç°**ï¼š
- âœ… å®Œæ•´dual-loopè®­ç»ƒï¼ˆmaskâ†’captionâ†’mask'â†’lossï¼‰
- âœ… Visual prompting caption generation
- âœ… VideoLLaVASAMModelé›†æˆ
- âœ… 4ä¸ªæ•°æ®é›†æ”¯æŒ
- âœ… é¢„è®­ç»ƒæƒé‡åŠ è½½
- âœ… åŸå§‹æ¶æ„å…¼å®¹

**å¯ä»¥ç›´æ¥ä½¿ç”¨**ï¼š
```bash
# æµ‹è¯•
bash test_dual_loop.sh

# ç¡®è®¤æ— è¯¯åï¼Œå®Œæ•´è®­ç»ƒ
bash run_dual_loop_full.sh
```

**ä¸éœ€è¦ä»»ä½•é¢å¤–ä¿®æ”¹**ï¼Œä»£ç å·²ç»å®Œæ•´å®ç°æ‰€æœ‰åŠŸèƒ½ï¼
